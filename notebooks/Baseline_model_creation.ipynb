{"cells":[{"cell_type":"markdown","metadata":{"id":"SVyanlHtNq7a"},"source":["Set as GPU before running."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16825,"status":"ok","timestamp":1764018993519,"user":{"displayName":"Jessie Kim","userId":"07761458504064228371"},"user_tz":300},"id":"nyhRlZbbNpc1","outputId":"deb76519-2dab-45f9-af23-7844f7e31aee"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28598,"status":"ok","timestamp":1764019022114,"user":{"displayName":"Jessie Kim","userId":"07761458504064228371"},"user_tz":300},"id":"nUSYMpnQOARa","outputId":"f53b5a55-0528-45fd-8a09-7d03b5ab091c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting face_recognition\n","  Downloading face_recognition-1.3.0-py2.py3-none-any.whl.metadata (21 kB)\n","Collecting face-recognition-models>=0.3.0 (from face_recognition)\n","  Downloading face_recognition_models-0.3.0.tar.gz (100.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.1/100.1 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.12/dist-packages (from face_recognition) (8.3.1)\n","Requirement already satisfied: dlib>=19.7 in /usr/local/lib/python3.12/dist-packages (from face_recognition) (19.24.6)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from face_recognition) (2.0.2)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from face_recognition) (11.3.0)\n","Downloading face_recognition-1.3.0-py2.py3-none-any.whl (15 kB)\n","Building wheels for collected packages: face-recognition-models\n","  Building wheel for face-recognition-models (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for face-recognition-models: filename=face_recognition_models-0.3.0-py2.py3-none-any.whl size=100566166 sha256=b9f7a45fa585e86fa4990793346357ae2eeac37caf054f866db1b23c3c32548d\n","  Stored in directory: /root/.cache/pip/wheels/8f/47/c8/f44c5aebb7507f7c8a2c0bd23151d732d0f0bd6884ad4ac635\n","Successfully built face-recognition-models\n","Installing collected packages: face-recognition-models, face_recognition\n","Successfully installed face-recognition-models-0.3.0 face_recognition-1.3.0\n"]}],"source":["!pip3 install face_recognition"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2591387,"status":"ok","timestamp":1764032510313,"user":{"displayName":"Jessie Kim","userId":"07761458504064228371"},"user_tz":300},"id":"Qr5LdSpnrZZg","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3f1224a7-bac5-4f87-e56f-6adfb91bf7ad"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 20 Validation Accuracy: 64.00%\n","Epoch 21 Validation Accuracy: 73.00%\n","Epoch 22 Validation Accuracy: 66.00%\n","Epoch 23 Validation Accuracy: 58.00%\n","Epoch 24 Validation Accuracy: 72.50%\n","Epoch 25 Validation Accuracy: 66.50%\n","Epoch 26 Validation Accuracy: 72.50%\n","Epoch 27 Validation Accuracy: 72.50%\n","Epoch 28 Validation Accuracy: 74.00%\n","Epoch 29 Validation Accuracy: 56.00%\n","Epoch 30 Validation Accuracy: 62.50%\n","Epoch 31 Validation Accuracy: 69.00%\n","Epoch 32 Validation Accuracy: 68.50%\n","Epoch 33 Validation Accuracy: 66.00%\n","Epoch 34 Validation Accuracy: 70.00%\n","Epoch 35 Validation Accuracy: 70.00%\n","Epoch 36 Validation Accuracy: 63.00%\n","Epoch 37 Validation Accuracy: 66.50%\n","Epoch 38 Validation Accuracy: 74.50%\n","Epoch 39 Validation Accuracy: 68.50%\n","Epoch 40 Validation Accuracy: 68.00%\n","Epoch 41 Validation Accuracy: 69.00%\n","Epoch 42 Validation Accuracy: 70.50%\n","Epoch 43 Validation Accuracy: 71.00%\n","Epoch 44 Validation Accuracy: 65.00%\n","Epoch 45 Validation Accuracy: 72.00%\n","Epoch 46 Validation Accuracy: 71.50%\n","Epoch 47 Validation Accuracy: 67.50%\n","Epoch 48 Validation Accuracy: 72.00%\n","Epoch 49 Validation Accuracy: 71.00%\n","Epoch 50 Validation Accuracy: 63.50%\n"]}],"source":["import os\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, models\n","from torch import nn\n","import pandas as pd\n","import cv2\n","\n","\n","class Model(nn.Module):\n","    def __init__(self, num_classes, latent_dim=2048, lstm_layers=1, hidden_dim=2048, bidirectional=False):\n","        super(Model, self).__init__()\n","        base_model = models.resnext50_32x4d(weights=None)\n","        self.cnn = nn.Sequential(*list(base_model.children())[:-2])\n","        self.avgpool = nn.AdaptiveAvgPool2d(1)\n","        self.lstm = nn.LSTM(latent_dim, hidden_dim, lstm_layers, bidirectional=bidirectional)\n","        self.dp = nn.Dropout(0.4)\n","        self.linear = nn.Linear(hidden_dim, num_classes)\n","\n","    def forward(self, x):\n","        b, seq_len, c, h, w = x.shape\n","        x = x.view(b * seq_len, c, h, w)\n","        fmap = self.cnn(x)\n","        x = self.avgpool(fmap)\n","        x = x.view(b, seq_len, 2048)\n","        x_lstm, _ = self.lstm(x)\n","        x = torch.mean(x_lstm, dim=1)\n","        return fmap, self.dp(self.linear(x))\n","\n","\n","class video_dataset(Dataset):\n","    def __init__(self, video_names, labels, sequence_length=20, transform=None):\n","        self.video_names = [v for v in video_names if os.path.exists(v)]\n","        self.labels = labels\n","        self.count = sequence_length\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.video_names)\n","\n","    def __getitem__(self, idx):\n","        video_path = self.video_names[idx]\n","        frames = []\n","        temp_video = os.path.basename(video_path)\n","        label = self.labels.loc[self.labels[\"file\"] == temp_video, \"label\"].values[0]\n","\n","        for frame in self.frame_extract(video_path):\n","            if frame is None:\n","                continue\n","            frames.append(self.transform(frame))\n","            if len(frames) == self.count:\n","                break\n","\n","        if len(frames) == 0:\n","            return self.__getitem__((idx + 1) % len(self.video_names))\n","\n","        frames = torch.stack(frames)\n","        if frames.shape[0] < self.count:\n","            pad_count = self.count - frames.shape[0]\n","            last_frame = frames[-1].unsqueeze(0).repeat(pad_count, 1, 1, 1)\n","            frames = torch.cat([frames, last_frame], dim=0)\n","\n","        return frames, label\n","\n","    def frame_extract(self, path):\n","        vidObj = cv2.VideoCapture(path)\n","        success = True\n","        while success:\n","            success, image = vidObj.read()\n","            if success:\n","                yield image\n","\n","\n","def read_list(txt_path):\n","    with open(txt_path, 'r') as f:\n","        files = [line.strip() for line in f.readlines()]\n","    return [f for f in files if os.path.exists(f)]\n","\n","def assign_label(path):\n","    path_low = path.lower()\n","    return 1 if any(x in path_low for x in [\"fake\", \"deepfake\", \"manipulated\"]) else 0\n","\n","def calculate_accuracy(outputs, targets):\n","    _, pred = outputs.max(1)\n","    return 100 * pred.eq(targets).sum().item() / targets.size(0)\n","\n","def test(model, data_loader, criterion):\n","    model.eval()\n","    accuracies = []\n","    with torch.no_grad():\n","        for inputs, targets in data_loader:\n","            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","            inputs, targets = inputs.to(device), targets.long().to(device)\n","            _, outputs = model(inputs)\n","            acc = calculate_accuracy(outputs, targets)\n","            accuracies.append(acc)\n","    return sum(accuracies) / len(accuracies)\n","\n","# Chosen Params\n","SEQ_LEN = 20\n","BATCH_SIZE = 4\n","IM_SIZE = 112\n","\n","val_txt = \"/content/drive/MyDrive/deepfake_detection_project/Dataset_split/baseline_splits/val.txt\"\n","val_files = read_list(val_txt)\n","val_labels = pd.DataFrame({\n","    \"file\": [os.path.basename(p) for p in val_files],\n","    \"label\": [assign_label(p) for p in val_files]\n","})\n","\n","val_transform = transforms.Compose([\n","    transforms.ToPILImage(),\n","    transforms.Resize((IM_SIZE, IM_SIZE)),\n","    transforms.ToTensor()\n","])\n","\n","val_dataset = video_dataset(val_files, val_labels, SEQ_LEN, transform=val_transform)\n","val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n","\n","# Checkpoints to evaluate\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","criterion = nn.CrossEntropyLoss().to(device)\n","checkpoints_dir = \"/content/drive/MyDrive/deepfake_detection_project/Baseline_model/models/\"\n","checkpoint_files = sorted([f for f in os.listdir(checkpoints_dir) if f.endswith(\".pt\")])\n","\n","checkpoint_files = sorted(checkpoint_files, key=lambda x : int(x.split('_')[-1].split('.')[0]))\n","\n","\n","val_acc_file = \"/content/drive/MyDrive/deepfake_detection_project/Baseline_model/val_accuracy.txt\"\n","\n","with open(val_acc_file, \"w\") as f_out:\n","    for ckpt_name in checkpoint_files:\n","        ckpt_path = os.path.join(checkpoints_dir, ckpt_name)\n","        checkpoint = torch.load(ckpt_path, map_location=device)\n","        model = Model(2).to(device)\n","        model.load_state_dict(checkpoint[\"model_state\"])\n","        val_acc = test(model, val_loader, criterion)\n","        print(f\"Epoch {int(ckpt_name.split('_')[-1].split('.')[0])} Validation Accuracy: {val_acc:.2f}%\")\n","        f_out.write(f\"Epoch {int(ckpt_name.split('_')[-1].split('.')[0])}: {val_acc:.2f}%\\n\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kkmfp08BN2Q9","outputId":"0660b4db-777f-49c2-e83d-2bc308341c54","executionInfo":{"status":"ok","timestamp":1764024941795,"user_tz":300,"elapsed":5919664,"user":{"displayName":"Jessie Kim","userId":"07761458504064228371"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Resuming training from epoch 40\n","[Epoch 40/50] [Batch 249/250] [Train Loss: 0.4929, Train Acc: 74.60%]\n","Epoch 40 Validation Accuracy: 68.00%\n","\n","Checkpoint saved: /content/drive/MyDrive/deepfake_detection_project/Baseline_model/models/model_epoch_40.pt\n","[Epoch 41/50] [Batch 249/250] [Train Loss: 0.4742, Train Acc: 76.10%]\n","Epoch 41 Validation Accuracy: 69.00%\n","\n","Checkpoint saved: /content/drive/MyDrive/deepfake_detection_project/Baseline_model/models/model_epoch_41.pt\n","[Epoch 42/50] [Batch 249/250] [Train Loss: 0.4793, Train Acc: 77.10%]\n","Epoch 42 Validation Accuracy: 70.50%\n","\n","Checkpoint saved: /content/drive/MyDrive/deepfake_detection_project/Baseline_model/models/model_epoch_42.pt\n","[Epoch 43/50] [Batch 249/250] [Train Loss: 0.4726, Train Acc: 75.50%]\n","Epoch 43 Validation Accuracy: 71.00%\n","\n","Checkpoint saved: /content/drive/MyDrive/deepfake_detection_project/Baseline_model/models/model_epoch_43.pt\n","[Epoch 44/50] [Batch 249/250] [Train Loss: 0.4813, Train Acc: 76.00%]\n","Epoch 44 Validation Accuracy: 65.00%\n","\n","Checkpoint saved: /content/drive/MyDrive/deepfake_detection_project/Baseline_model/models/model_epoch_44.pt\n","[Epoch 45/50] [Batch 249/250] [Train Loss: 0.4696, Train Acc: 77.40%]\n","Epoch 45 Validation Accuracy: 72.00%\n","\n","Checkpoint saved: /content/drive/MyDrive/deepfake_detection_project/Baseline_model/models/model_epoch_45.pt\n","[Epoch 46/50] [Batch 249/250] [Train Loss: 0.4392, Train Acc: 78.70%]\n","Epoch 46 Validation Accuracy: 71.50%\n","\n","Checkpoint saved: /content/drive/MyDrive/deepfake_detection_project/Baseline_model/models/model_epoch_46.pt\n","[Epoch 47/50] [Batch 249/250] [Train Loss: 0.4256, Train Acc: 79.50%]\n","Epoch 47 Validation Accuracy: 67.50%\n","\n","Checkpoint saved: /content/drive/MyDrive/deepfake_detection_project/Baseline_model/models/model_epoch_47.pt\n","[Epoch 48/50] [Batch 249/250] [Train Loss: 0.4821, Train Acc: 74.70%]\n","Epoch 48 Validation Accuracy: 72.00%\n","\n","Checkpoint saved: /content/drive/MyDrive/deepfake_detection_project/Baseline_model/models/model_epoch_48.pt\n","[Epoch 49/50] [Batch 249/250] [Train Loss: 0.4552, Train Acc: 78.10%]\n","Epoch 49 Validation Accuracy: 71.00%\n","\n","Checkpoint saved: /content/drive/MyDrive/deepfake_detection_project/Baseline_model/models/model_epoch_49.pt\n","[Epoch 50/50] [Batch 249/250] [Train Loss: 0.4642, Train Acc: 76.20%]\n","Epoch 50 Validation Accuracy: 63.50%\n","\n","Checkpoint saved: /content/drive/MyDrive/deepfake_detection_project/Baseline_model/models/model_epoch_50.pt\n"]}],"source":["import os\n","import numpy as np\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, models\n","from torch import nn\n","from sklearn.metrics import f1_score, roc_curve, auc, confusion_matrix\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import seaborn as sn\n","import cv2\n","import sys\n","\n","\n","# Model Definition\n","class Model(nn.Module):\n","    def __init__(self, num_classes, latent_dim=2048, lstm_layers=1, hidden_dim=2048, bidirectional=False):\n","        super(Model, self).__init__()\n","        base_model = models.resnext50_32x4d(weights=None)\n","        self.cnn = nn.Sequential(*list(base_model.children())[:-2])\n","        self.avgpool = nn.AdaptiveAvgPool2d(1)\n","        self.lstm = nn.LSTM(latent_dim, hidden_dim, lstm_layers, bidirectional=bidirectional)\n","        self.dp = nn.Dropout(0.4)\n","        self.linear = nn.Linear(hidden_dim, num_classes)\n","\n","    def forward(self, x):\n","        batch_size, seq_length, c, h, w = x.shape\n","        x = x.view(batch_size * seq_length, c, h, w)\n","        fmap = self.cnn(x)\n","        x = self.avgpool(fmap)\n","        x = x.view(batch_size, seq_length, 2048)\n","        x_lstm, _ = self.lstm(x)\n","        x = torch.mean(x_lstm, dim=1)\n","        return fmap, self.dp(self.linear(x))\n","\n","# Dataset with padding and skip zero frames\n","class video_dataset(Dataset):\n","    def __init__(self, video_names, labels, sequence_length=20, transform=None):\n","        self.video_names = video_names\n","        self.labels = labels\n","        self.count = sequence_length\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.video_names)\n","\n","    def __getitem__(self, idx):\n","        video_path = self.video_names[idx]\n","        frames = []\n","        temp_video = os.path.basename(video_path)\n","        label = self.labels.loc[self.labels[\"file\"] == temp_video, \"label\"].values[0]\n","\n","        for frame in self.frame_extract(video_path):\n","            frames.append(self.transform(frame))\n","            if len(frames) == self.count:\n","                break\n","\n","        if len(frames) == 0:\n","            # Skip videos with 0 frames\n","            return self.__getitem__((idx + 1) % len(self.video_names))\n","\n","        frames = torch.stack(frames)\n","\n","        # Pad short videos by repeating last frame\n","        if frames.shape[0] < self.count:\n","            pad_count = self.count - frames.shape[0]\n","            last_frame = frames[-1].unsqueeze(0).repeat(pad_count, 1, 1, 1)\n","            frames = torch.cat([frames, last_frame], dim=0)\n","\n","        return frames, label\n","\n","    def frame_extract(self, path):\n","        vidObj = cv2.VideoCapture(path)\n","        success = True\n","        while success:\n","            success, image = vidObj.read()\n","            if success:\n","                yield image\n","\n","# Utilities\n","def train_epoch(epoch, num_epochs, data_loader, model, criterion, optimizer):\n","    model.train()\n","    losses = AverageMeter()\n","    accuracies = AverageMeter()\n","    for i, (inputs, targets) in enumerate(data_loader):\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        inputs, targets = inputs.to(device), targets.long().to(device)\n","        _, outputs = model(inputs)\n","        loss = criterion(outputs, targets)\n","        acc = calculate_accuracy(outputs, targets)\n","        losses.update(loss.item(), inputs.size(0))\n","        accuracies.update(acc, inputs.size(0))\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        sys.stdout.write(\n","            f\"\\r[Epoch {epoch}/{num_epochs}] [Batch {i}/{len(data_loader)}] [Train Loss: {losses.avg:.4f}, Train Acc: {accuracies.avg:.2f}%]\"\n","        )\n","    print()\n","    return losses.avg, accuracies.avg\n","\n","def test(epoch, model, data_loader, criterion):\n","    model.eval()\n","    losses = AverageMeter()\n","    accuracies = AverageMeter()\n","    pred, true, probs_list = [], [], []\n","    with torch.no_grad():\n","        for i, (inputs, targets) in enumerate(data_loader):\n","            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","            inputs, targets = inputs.to(device), targets.long().to(device)\n","            _, outputs = model(inputs)\n","            probs = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n","            loss = criterion(outputs, targets)\n","            acc = calculate_accuracy(outputs, targets)\n","            _, p = torch.max(outputs, 1)\n","            true += targets.cpu().numpy().tolist()\n","            pred += p.cpu().numpy().tolist()\n","            probs_list.extend(probs.tolist())\n","            losses.update(loss.item(), inputs.size(0))\n","            accuracies.update(acc, inputs.size(0))\n","    return true, pred, probs_list, losses.avg, accuracies.avg\n","\n","def calculate_accuracy(outputs, targets):\n","    _, pred = outputs.max(1)\n","    correct = pred.eq(targets).sum().item()\n","    return 100 * correct / targets.size(0)\n","\n","class AverageMeter():\n","    def __init__(self):\n","        self.reset()\n","    def reset(self):\n","        self.val = self.avg = self.sum = self.count = 0\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","def save_checkpoint(state, filename):\n","    os.makedirs(os.path.dirname(filename), exist_ok=True)\n","    torch.save(state, filename)\n","    print(f\"\\nCheckpoint saved: {filename}\")\n","\n","def read_list(txt_path):\n","    with open(txt_path, 'r') as f:\n","        return [line.strip() for line in f.readlines()]\n","\n","def assign_label(path):\n","    path_low = path.lower()\n","    if \"fake\" in path_low or \"deepfake\" in path_low or \"manipulated\" in path_low:\n","        return 1\n","    return 0\n","\n","\n","SEQ_LEN = 20\n","BATCH_SIZE = 4\n","NUM_EPOCHS = 50 # update to the max epoch you want\n","LR = 1e-5\n","IM_SIZE = 112\n","\n","# Dataset splits\n","train_files = read_list(\"/content/drive/MyDrive/deepfake_detection_project/Dataset_split/baseline_splits/train.txt\")\n","val_files = read_list(\"/content/drive/MyDrive/deepfake_detection_project/Dataset_split/baseline_splits/val.txt\")\n","\n","train_labels = pd.DataFrame({\"file\":[os.path.basename(p) for p in train_files],\n","                             \"label\":[assign_label(p) for p in train_files]})\n","val_labels = pd.DataFrame({\"file\":[os.path.basename(p) for p in val_files],\n","                           \"label\":[assign_label(p) for p in val_files]})\n","\n","# Transforms\n","train_transform = transforms.Compose([transforms.ToPILImage(),\n","                                      transforms.Resize((IM_SIZE, IM_SIZE)),\n","                                      transforms.ToTensor()])\n","val_transform = train_transform\n","\n","# Dataset and loaders\n","train_dataset = video_dataset(train_files, train_labels, SEQ_LEN, transform=train_transform)\n","val_dataset   = video_dataset(val_files, val_labels, SEQ_LEN, transform=val_transform)\n","\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n","val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n","\n","# Model, optimizer, criterion\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = Model(2).to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n","criterion = nn.CrossEntropyLoss().to(device)\n","\n","# Load checkpoint\n","checkpoint_path = \"/content/drive/MyDrive/deepfake_detection_project/Baseline_model/models/model_epoch_39.pt\"\n","checkpoint = torch.load(checkpoint_path, map_location=device)\n","model.load_state_dict(checkpoint[\"model_state\"])\n","optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n","start_epoch = checkpoint[\"epoch\"] + 1  # continue from next epoch\n","\n","print(f\"Resuming training from epoch {start_epoch}\")\n","\n","\n","# Training loop\n","train_loss_avg, train_acc_list = [], []\n","val_loss_avg, val_acc_list = [], []\n","\n","val_acc_file = \"/content/drive/MyDrive/deepfake_detection_project/Baseline_model/val_accuracy.txt\"\n","\n","for epoch in range(start_epoch, NUM_EPOCHS + 1):\n","    train_loss, train_acc = train_epoch(epoch, NUM_EPOCHS, train_loader, model, criterion, optimizer)\n","    train_loss_avg.append(train_loss)\n","    train_acc_list.append(train_acc)\n","\n","    y_true, y_pred, y_probs, val_loss, val_acc = test(epoch, model, val_loader, criterion)\n","    val_loss_avg.append(val_loss)\n","    val_acc_list.append(val_acc)\n","\n","    # Print and save validation accuracy\n","    print(f\"Epoch {epoch} Validation Accuracy: {val_acc:.2f}%\")\n","    with open(val_acc_file, \"a\") as f:\n","        f.write(f\"Epoch {epoch}: {val_acc:.2f}%\\n\")\n","\n","    # Save model checkpoint\n","    save_checkpoint({\n","        \"epoch\": epoch,\n","        \"model_state\": model.state_dict(),\n","        \"optimizer_state\": optimizer.state_dict(),\n","        \"train_loss\": train_loss,\n","        \"train_acc\": train_acc,\n","        \"val_loss\": val_loss,\n","        \"val_acc\": val_acc\n","    }, filename=f\"/content/drive/MyDrive/deepfake_detection_project/Baseline_model/models/model_epoch_{epoch}.pt\")\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1a6ryD9-lSd-bjwcqmlWfSJmmP0kR5cuh","timestamp":1763879539551}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}