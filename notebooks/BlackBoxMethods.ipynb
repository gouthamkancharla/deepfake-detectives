{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1OWznl133yDLcQI8c1MI43FQS_fT1nMLM","authorship_tag":"ABX9TyMC0NtvBk96/lomU5J7jOUY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# Installing dependencies\n","!pip3 install opencv-python\n","!pip3 install numpy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z5FTEyElJyRi","executionInfo":{"status":"ok","timestamp":1762265810989,"user_tz":300,"elapsed":11486,"user":{"displayName":"Marcus Lai","userId":"03480055170255257153"}},"outputId":"c68f27f0-5c83-4cb4-dea2-476076583646"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n","Requirement already satisfied: numpy<2.3.0,>=2 in /usr/local/lib/python3.12/dist-packages (from opencv-python) (2.0.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n"]}]},{"cell_type":"markdown","source":["Cloning repository of all black box adversarial attack methods."],"metadata":{"id":"wQ9c7tpMffi0"}},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yxmle4iafCYc","executionInfo":{"status":"ok","timestamp":1762265811098,"user_tz":300,"elapsed":104,"user":{"displayName":"Marcus Lai","userId":"03480055170255257153"}},"outputId":"04abd651-ff76-4f02-cba7-3f9252af84b7"},"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'BlackboxBench' already exists and is not an empty directory.\n"]}],"source":["!git clone https://github.com/SCLBD/BlackboxBench.git"]},{"cell_type":"markdown","source":["See if we can just plug our dataset into the methods and run it."],"metadata":{"id":"WzNrg_ZthSSm"}},{"cell_type":"markdown","source":["Load Drive and test example on it"],"metadata":{"id":"6_pb_X6OQmgA"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"id":"WM0isYdOOJUs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762265814954,"user_tz":300,"elapsed":3852,"user":{"displayName":"Marcus Lai","userId":"03480055170255257153"}},"outputId":"cdcb09fe-4dde-4e38-da33-de193aa6a1f5"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%%writefile /content/my_detector_module.py\n","import torch\n","from torch import nn\n","from torchvision import models, transforms\n","import numpy as np\n","import cv2\n","\n","# replicate your model definition\n","class Model(nn.Module):\n","    def __init__(self, num_classes, latent_dim=2048, lstm_layers=1, hidden_dim=2048, bidirectional=False):\n","        super(Model, self).__init__()\n","        model = models.resnext50_32x4d(pretrained=True)\n","        self.model = nn.Sequential(*list(model.children())[:-2])\n","        self.lstm = nn.LSTM(latent_dim, hidden_dim, lstm_layers, bidirectional)\n","        self.relu = nn.LeakyReLU()\n","        self.dp = nn.Dropout(0.4)\n","        self.linear1 = nn.Linear(2048, num_classes)\n","        self.avgpool = nn.AdaptiveAvgPool2d(1)\n","\n","    def forward(self, x):\n","        batch_size, seq_length, c, h, w = x.shape\n","        x = x.view(batch_size * seq_length, c, h, w)\n","        fmap = self.model(x)\n","        x = self.avgpool(fmap)\n","        x = x.view(batch_size, seq_length, 2048)\n","        x_lstm, _ = self.lstm(x, None)\n","        return fmap, self.dp(self.linear1(x_lstm[:, -1, :]))\n","\n","# checkpoint path (change this to your Drive model path)\n","_CHECKPOINT_PATH = \"/content/drive/My Drive/Models/model_87_acc_20_frames_final_data.pt\"\n","\n","# instantiate model\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","_model = Model(2).to(device)\n","_model.load_state_dict(torch.load(_CHECKPOINT_PATH, map_location=device))\n","_model.eval()\n","\n","# same preprocessing as in your notebook\n","mean = [0.485, 0.456, 0.406]\n","std = [0.229, 0.224, 0.225]\n","_transform = transforms.Compose([\n","    transforms.ToPILImage(),\n","    transforms.Resize((112, 112)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean, std)\n","])\n","_sm = nn.Softmax(dim=1)\n","\n","# prediction wrapper for attack\n","def predict_batch(np_batch):\n","    \"\"\"\n","    np_batch: numpy array (N,H,W,3) uint8\n","    returns: list of predicted labels (0=fake,1=real)\n","    \"\"\"\n","    results = []\n","    with torch.no_grad():\n","        for frame in np_batch:\n","            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","            tensor = _transform(frame).unsqueeze(0).unsqueeze(0).to(device)\n","            fmap, logits = _model(tensor)\n","            probs = _sm(logits)\n","            preds = torch.argmax(probs, dim=1)\n","            results.append(preds.item())\n","    return np.array(results)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lMv_XZb5JKIo","executionInfo":{"status":"ok","timestamp":1762265814982,"user_tz":300,"elapsed":25,"user":{"displayName":"Marcus Lai","userId":"03480055170255257153"}},"outputId":"9646b98a-a452-4c80-cb08-42f24be5ad61"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting /content/my_detector_module.py\n"]}]},{"cell_type":"markdown","source":["All in one"],"metadata":{"id":"99GBizaSTnfX"}},{"cell_type":"code","source":["# == Plug-and-play cell: run your repo's BoundaryAttack on first 70 frames using your trained detector ==\n","REPO_PATH = \"/content/BlackboxBench/query\"\n","MODULE_WITH_PREDICT = \"/content/my_detector_module.py\"  # change path if stored in Drive\n","\n","# ---------------------------- do not change below ----------------------------\n","%pylab inline\n","import os, sys, importlib.util, traceback\n","from typing import List, Tuple\n","import numpy as np\n","import cv2\n","import torch\n","import torch.nn.functional as F\n","\n","try:\n","    import torchvision\n","    import torchvision.transforms.functional as TF\n","except Exception:\n","    torchvision = None\n","    TF = None\n","\n","def ensure_package(root):\n","    for d in [\"\", \"attacks\", \"attacks/decision\"]:\n","        p = os.path.join(root, d)\n","        if os.path.isdir(p):\n","            init = os.path.join(p, \"__init__.py\")\n","            if not os.path.exists(init):\n","                open(init, \"a\").close()\n","ensure_package(REPO_PATH)\n","\n","if REPO_PATH not in sys.path:\n","    sys.path.insert(0, REPO_PATH)\n","\n","boundary_attack_path = os.path.join(REPO_PATH, \"attacks/decision/boundary_attack.py\")\n","BoundaryAttack = None\n","if os.path.exists(boundary_attack_path):\n","    spec = importlib.util.spec_from_file_location(\"boundary_attack\", boundary_attack_path)\n","    module = importlib.util.module_from_spec(spec)\n","    try:\n","        sys.modules[\"boundary_attack\"] = module\n","        spec.loader.exec_module(module)\n","        BoundaryAttack = getattr(module, \"BoundaryAttack\", None)\n","        print(\"Imported BoundaryAttack:\", BoundaryAttack)\n","    except Exception as e:\n","        print(\"Failed to import boundary_attack module:\", e)\n","        traceback.print_exc()\n","else:\n","    print(\"boundary_attack.py not found at:\", boundary_attack_path)\n","\n","def safe_get_fps(cap):\n","    fps = cap.get(cv2.CAP_PROP_FPS)\n","    if fps is None or fps <= 0 or np.isnan(fps):\n","        return 25.0\n","    return float(fps)\n","\n","def Process_Video(INPUT_VIDEO, OUTPUT_VIDEO, NUM_CHANGE):\n","    if not os.path.exists(INPUT_VIDEO):\n","        raise FileNotFoundError(\"Upload your input video to /content/ and set INPUT_VIDEO path.\")\n","\n","    cap = cv2.VideoCapture(INPUT_VIDEO)\n","    if not cap.isOpened():\n","        raise RuntimeError(\"Failed to open input video.\")\n","    fps = safe_get_fps(cap)\n","    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","    frames_bgr = []\n","    while True:\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","        frames_bgr.append(frame)\n","    cap.release()\n","    print(f\"Loaded {len(frames_bgr)} frames (W x H = {w} x {h}), fps={fps}\")\n","\n","    attack = None\n","    if BoundaryAttack is not None:\n","        try:\n","            attack = BoundaryAttack(\n","                epsilon=1e-1,\n","                p='2',\n","                max_queries=5,\n","                lb=0.0,\n","                ub=255.0,\n","                batch_size=2,\n","                steps=5,\n","                spherical_step=0.1,\n","                source_step=0.01,\n","                source_step_convergance=1e-4,\n","                step_adaptation=1.5,\n","                update_stats_every_k=10,\n","            )\n","            print(\"Instantiated BoundaryAttack.\")\n","        except Exception as e:\n","            print(\"Failed to instantiate BoundaryAttack:\", e)\n","            traceback.print_exc()\n","            attack = None\n","    else:\n","        print(\"BoundaryAttack not available; will fallback per-frame.\")\n","\n","    # --------------------------------------------------------------------\n","    # use your trained model module instead of ResNet18\n","    # --------------------------------------------------------------------\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model_input_size = (224, 224)\n","\n","    def model_predict_labels_from_candidate_tensor(candidate_tensor):\n","        # dynamically import your detector\n","        import importlib.util\n","        spec = importlib.util.spec_from_file_location(\"user_detector_module\", MODULE_WITH_PREDICT)\n","        user_mod = importlib.util.module_from_spec(spec)\n","        spec.loader.exec_module(user_mod)\n","\n","        if isinstance(candidate_tensor, torch.Tensor):\n","            np_batch = candidate_tensor.detach().cpu().numpy()\n","        else:\n","            np_batch = np.asarray(candidate_tensor)\n","        if np_batch.ndim == 3:\n","            np_batch = np_batch[None, ...]\n","        if np_batch.dtype != np.uint8:\n","            if np_batch.max() <= 1.0:\n","                np_batch = (np_batch * 255.0).clip(0,255).astype(np.uint8)\n","            else:\n","                np_batch = np.clip(np_batch, 0,255).astype(np.uint8)\n","\n","        if hasattr(user_mod, \"predict_batch\"):\n","            out = user_mod.predict_batch(np_batch)\n","        elif hasattr(user_mod, \"predict\"):\n","            out = user_mod.predict(np_batch)\n","        else:\n","            raise RuntimeError(\"Module must provide predict_batch(np_batch) or predict(np_batch).\")\n","\n","        out = np.asarray(out)\n","        if out.ndim == 1 and np.issubdtype(out.dtype, np.floating):\n","            labels = (out > 0.5).astype(np.int64)\n","        elif out.ndim == 2 and out.shape[1] == 2:\n","            labels = np.argmax(out, axis=1).astype(np.int64)\n","        else:\n","            labels = out.astype(np.int64).reshape(-1)\n","\n","        return torch.from_numpy(labels).long()\n","\n","    def batch_to_model_input(x_batch, target_size=model_input_size):\n","        if isinstance(x_batch, np.ndarray):\n","            x = torch.from_numpy(x_batch.astype(np.float32))\n","        else:\n","            x = x_batch.float()\n","        x = x.to(device)\n","        x = x.permute(0,3,1,2)\n","        x = x / 255.0\n","        x = F.interpolate(x, size=target_size, mode=\"bilinear\", align_corners=False)\n","        mean = torch.tensor([0.485, 0.456, 0.406], device=device).view(1,3,1,1)\n","        std  = torch.tensor([0.229, 0.224, 0.225], device=device).view(1,3,1,1)\n","        x = (x - mean) / std\n","        return x\n","\n","    if attack is not None:\n","        def is_adversarial_fn(candidates, labels):\n","            if isinstance(candidates, torch.Tensor):\n","                cand = candidates.detach().cpu().numpy()\n","            else:\n","                cand = np.asarray(candidates)\n","            preds = model_predict_labels_from_candidate_tensor(cand)\n","            if isinstance(labels, torch.Tensor):\n","                labs = labels.detach().cpu().long()\n","            else:\n","                labs = torch.tensor(labels, dtype=torch.long)\n","            return (preds != labs).to(torch.bool)\n","\n","        def distance_fn(a, b):\n","            if not isinstance(a, torch.Tensor):\n","                a = torch.from_numpy(np.asarray(a).astype(np.float32))\n","            if not isinstance(b, torch.Tensor):\n","                b = torch.from_numpy(np.asarray(b).astype(np.float32))\n","            a = a.view(a.shape[0], -1)\n","            b = b.view(b.shape[0], -1)\n","            return torch.norm(a - b, dim=1)\n","\n","        attack.is_adversarial = is_adversarial_fn\n","        attack.distance = distance_fn\n","        print(\"Patched attack.is_adversarial and attack.distance to use your detector.\")\n","    else:\n","        print(\"Attack object not available.\")\n","\n","    # --------------------------------------------------------------------\n","    # process frames\n","    # --------------------------------------------------------------------\n","    out_frames = []\n","    changed = 0\n","\n","    for i, frame_bgr in enumerate(frames_bgr):\n","        if i < NUM_CHANGE and attack is not None:\n","            try:\n","                frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n","                xs = torch.from_numpy(frame_rgb.astype(np.float32)).unsqueeze(0)\n","                orig_label = model_predict_labels_from_candidate_tensor(xs)\n","                ys = orig_label\n","                adv, q = attack._perturb(xs, ys)\n","                if isinstance(adv, torch.Tensor):\n","                    adv_np = adv.detach().cpu().numpy()\n","                else:\n","                    adv_np = np.asarray(adv)\n","                if adv_np.ndim == 4 and adv_np.shape[0] == 1:\n","                    adv_np = adv_np[0]\n","                adv_np = np.clip(adv_np, 0, 255).astype(np.uint8)\n","                out_bgr = cv2.cvtColor(adv_np, cv2.COLOR_RGB2BGR)\n","                out_frames.append(out_bgr)\n","                changed += 1\n","                print(f\"Frame {i}: attack applied (q={q}).\")\n","            except Exception as e:\n","                print(f\"Frame {i}: attack failed -> fallback invert. Error: {e}\")\n","                traceback.print_exc()\n","                out_frames.append(255 - frame_bgr)\n","        else:\n","            break\n","\n","    print(f\"Processed {len(out_frames)} frames; changed {changed} frames.\")\n","\n","    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n","    writer = cv2.VideoWriter(OUTPUT_VIDEO, fourcc, fps, (w, h))\n","    if not writer.isOpened():\n","        raise RuntimeError(\"Failed to open VideoWriter for output.\")\n","    for f in out_frames:\n","        writer.write(f)\n","    writer.release()\n","    print(\"Saved output video to:\", OUTPUT_VIDEO)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s6LQnMCLGp-V","executionInfo":{"status":"ok","timestamp":1762265827231,"user_tz":300,"elapsed":12249,"user":{"displayName":"Marcus Lai","userId":"03480055170255257153"}},"outputId":"4e232c68-72ad-4c20-e0c8-abde4c2e1af0"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Populating the interactive namespace from numpy and matplotlib\n","Imported BoundaryAttack: <class 'boundary_attack.BoundaryAttack'>\n"]}]},{"cell_type":"code","source":["# # == Plug-and-play cell: run your repo's BoundaryAttack on first 70 frames ==\n","# REPO_PATH = \"/content/BlackboxBench/query\"\n","\n","\n","# # ---------------------------- do not change below ----------------------------\n","# import os, sys, importlib.util, traceback\n","# from typing import List, Tuple\n","# import numpy as np\n","# import cv2\n","# import torch\n","# import torch.nn.functional as F\n","\n","# # Try to import torchvision for a pretrained decision oracle\n","# try:\n","#     import torchvision\n","#     import torchvision.transforms.functional as TF\n","# except Exception:\n","#     torchvision = None\n","#     TF = None\n","\n","# # ensure repo packages have __init__.py so imports work\n","# def ensure_package(root):\n","#     for d in [\"\", \"attacks\", \"attacks/decision\"]:\n","#         p = os.path.join(root, d)\n","#         if os.path.isdir(p):\n","#             init = os.path.join(p, \"__init__.py\")\n","#             if not os.path.exists(init):\n","#                 open(init, \"a\").close()\n","# ensure_package(REPO_PATH)\n","\n","# # make repo importable\n","# if REPO_PATH not in sys.path:\n","#     sys.path.insert(0, REPO_PATH)\n","\n","# # attempt to import boundary_attack.py\n","# boundary_attack_path = os.path.join(REPO_PATH, \"attacks/decision/boundary_attack.py\")\n","# BoundaryAttack = None\n","# if os.path.exists(boundary_attack_path):\n","#     spec = importlib.util.spec_from_file_location(\"boundary_attack\", boundary_attack_path)\n","#     module = importlib.util.module_from_spec(spec)\n","#     try:\n","#         sys.modules[\"boundary_attack\"] = module\n","#         spec.loader.exec_module(module)\n","#         BoundaryAttack = getattr(module, \"BoundaryAttack\", None)\n","#         print(\"Imported BoundaryAttack:\", BoundaryAttack)\n","#     except Exception as e:\n","#         print(\"Failed to import boundary_attack module:\", e)\n","#         traceback.print_exc()\n","# else:\n","#     print(\"boundary_attack.py not found at:\", boundary_attack_path)\n","\n","# # load video frames\n","# def safe_get_fps(cap):\n","#     fps = cap.get(cv2.CAP_PROP_FPS)\n","#     if fps is None or fps <= 0 or np.isnan(fps):\n","#         return 25.0\n","#     return float(fps)\n","\n","# def Process_Video(INPUT_VIDEO, OUTPUT_VIDEO, NUM_CHANGE):\n","#   if not os.path.exists(INPUT_VIDEO):\n","#       raise FileNotFoundError(\"Upload your input video to /content/ and set INPUT_VIDEO path.\")\n","\n","#   cap = cv2.VideoCapture(INPUT_VIDEO)\n","#   if not cap.isOpened():\n","#       raise RuntimeError(\"Failed to open input video.\")\n","#   fps = safe_get_fps(cap)\n","#   w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","#   h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","#   frames_bgr = []\n","#   while True:\n","#       ret, frame = cap.read()\n","#       if not ret:\n","#           break\n","#       frames_bgr.append(frame)\n","#   cap.release()\n","#   print(f\"Loaded {len(frames_bgr)} frames (W x H = {w} x {h}), fps={fps}\")\n","\n","#   # prepare attack instance (use string '2' for p)\n","#   attack = None\n","#   if BoundaryAttack is not None:\n","#       try:\n","#           attack = BoundaryAttack(\n","#               epsilon=1e-2,\n","#               p='2',                    # <- IMPORTANT: string '2'\n","#               max_queries=10,\n","#               lb=0.0,\n","#               ub=255.0,\n","#               batch_size=3,\n","#               steps=5,\n","#               spherical_step=0.1,\n","#               source_step=0.01,\n","#               source_step_convergance=1e-4,\n","#               step_adaptation=1.5,\n","#               update_stats_every_k=10,\n","#           )\n","#           print(\"Instantiated BoundaryAttack.\")\n","#       except Exception as e:\n","#           print(\"Failed to instantiate BoundaryAttack:\", e)\n","#           traceback.print_exc()\n","#           attack = None\n","#   else:\n","#       print(\"BoundaryAttack not available; will fallback per-frame.\")\n","\n","#   # load pretrained model as decision oracle (ResNet18 ImageNet)\n","#   device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","#   model = None\n","#   model_input_size = (224, 224)\n","#   if torchvision is not None:\n","#       try:\n","#           model = torchvision.models.resnet18(pretrained=True).to(device).eval()\n","#           print(\"Loaded torchvision resnet18 as decision oracle.\")\n","#       except Exception as e:\n","#           print(\"Failed to load pretrained model:\", e)\n","#           model = None\n","#   else:\n","#       print(\"torchvision not available; attack will fallback.\")\n","\n","#   # helpers to convert candidates to model inputs & predict labels\n","#   def batch_to_model_input(x_batch, target_size=model_input_size):\n","#       # x_batch: torch tensor shape (N,H,W,3) in [0,255] or numpy array\n","#       if isinstance(x_batch, np.ndarray):\n","#           x = torch.from_numpy(x_batch.astype(np.float32))\n","#       else:\n","#           x = x_batch.float()\n","#       x = x.to(device)\n","#       x = x.permute(0,3,1,2)  # (N,3,H,W)\n","#       x = x / 255.0\n","#       x = F.interpolate(x, size=target_size, mode=\"bilinear\", align_corners=False)\n","#       mean = torch.tensor([0.485, 0.456, 0.406], device=device).view(1,3,1,1)\n","#       std  = torch.tensor([0.229, 0.224, 0.225], device=device).view(1,3,1,1)\n","#       x = (x - mean) / std\n","#       return x\n","\n","#   def model_predict_labels_from_candidate_tensor(candidate_tensor):\n","#       # accepts torch tensor (N,H,W,3) or numpy, returns torch.LongTensor (N,)\n","#       if model is None:\n","#           raise RuntimeError(\"No model available for predictions.\")\n","#       was_numpy = isinstance(candidate_tensor, np.ndarray)\n","#       if was_numpy:\n","#           cand = torch.from_numpy(candidate_tensor.astype(np.float32)).to(device)\n","#       else:\n","#           cand = candidate_tensor.to(device).float()\n","#       x = batch_to_model_input(cand, target_size=model_input_size)\n","#       with torch.no_grad():\n","#           logits = model(x)\n","#           preds = torch.argmax(logits, dim=1)\n","#       return preds.cpu()\n","\n","#   # monkey-patch attack.is_adversarial and attack.distance if attack exists\n","#   if attack is not None and model is not None:\n","#       def is_adversarial_fn(candidates, labels):\n","#           # candidates: torch Tensor (N,H,W,3) or numpy -> returns torch.BoolTensor (N,)\n","#           if isinstance(candidates, torch.Tensor):\n","#               cand = candidates.detach().cpu()\n","#           else:\n","#               cand = np.asarray(candidates)\n","#           preds = model_predict_labels_from_candidate_tensor(cand)\n","#           if isinstance(labels, torch.Tensor):\n","#               labs = labels.detach().cpu().long()\n","#           else:\n","#               labs = torch.tensor(labels, dtype=torch.long)\n","#           return (preds != labs).to(torch.bool)\n","\n","#       def distance_fn(a, b):\n","#           # L2 distance per-sample; a,b are torch tensors or numpy\n","#           if not isinstance(a, torch.Tensor):\n","#               a = torch.from_numpy(np.asarray(a).astype(np.float32))\n","#           if not isinstance(b, torch.Tensor):\n","#               b = torch.from_numpy(np.asarray(b).astype(np.float32))\n","#           a = a.view(a.shape[0], -1)\n","#           b = b.view(b.shape[0], -1)\n","#           return torch.norm(a - b, dim=1)\n","\n","#       attack.is_adversarial = is_adversarial_fn\n","#       attack.distance = distance_fn\n","#       print(\"Patched attack.is_adversarial and attack.distance.\")\n","#   else:\n","#       print(\"Attack or model not available -> falling back per-frame where needed.\")\n","\n","#   # process frames: attempt attack on first NUM_CHANGE frames\n","#   out_frames = []\n","#   changed = 0\n","\n","#   for i, frame_bgr in enumerate(frames_bgr):\n","#       if i < NUM_CHANGE and attack is not None and model is not None:\n","#           try:\n","#               frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n","#               xs = torch.from_numpy(frame_rgb.astype(np.float32)).unsqueeze(0)  # (1,H,W,3)\n","#               # get the model's predicted label for the original frame (so attack tries to change it)\n","#               orig_label = model_predict_labels_from_candidate_tensor(xs)  # (1,)\n","#               ys = orig_label  # use predicted label as the \"true\" label\n","#               adv, q = attack._perturb(xs, ys)  # call repo attack\n","#               # convert adv to numpy HWC uint8\n","#               if isinstance(adv, torch.Tensor):\n","#                   adv_np = adv.detach().cpu().numpy()\n","#               else:\n","#                   adv_np = np.asarray(adv)\n","#               if adv_np.ndim == 4 and adv_np.shape[0] == 1:\n","#                   adv_np = adv_np[0]\n","#               adv_np = np.clip(adv_np, 0, 255).astype(np.uint8)\n","#               out_bgr = cv2.cvtColor(adv_np, cv2.COLOR_RGB2BGR)\n","#               out_frames.append(out_bgr)\n","#               changed += 1\n","#               print(f\"Frame {i}: attack applied (q={q}).\")\n","#           except Exception as e:\n","#               print(f\"Frame {i}: attack failed -> fallback invert. Error: {e}\")\n","#               traceback.print_exc()\n","#               out_frames.append(255 - frame_bgr)\n","#       else:\n","#           break\n","\n","#   print(f\"Processed {len(out_frames)} frames; changed {changed} frames.\")\n","\n","#   # write output video\n","#   fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n","#   writer = cv2.VideoWriter(OUTPUT_VIDEO, fourcc, fps, (w, h))\n","#   if not writer.isOpened():\n","#       raise RuntimeError(\"Failed to open VideoWriter for output.\")\n","#   for f in out_frames:\n","#       writer.write(f)\n","#   writer.release()\n","#   print(\"Saved output video to:\", OUTPUT_VIDEO)\n"],"metadata":{"id":"GedF6FKYTmy5","executionInfo":{"status":"ok","timestamp":1762265827288,"user_tz":300,"elapsed":31,"user":{"displayName":"Marcus Lai","userId":"03480055170255257153"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["import os\n","import glob\n","from tqdm import tqdm"],"metadata":{"id":"3PiLhcnBkm7Y","executionInfo":{"status":"ok","timestamp":1762265827297,"user_tz":300,"elapsed":7,"user":{"displayName":"Marcus Lai","userId":"03480055170255257153"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Set these paths before running:\n","REPO_PATH = \"/content/BlackboxBench/query\"\n","src_dir = \"/content/drive/MyDrive/faceforensics++/manipulated_sequences/DeepFakeDetection/c40/videos/\"\n","output_dir = \"/content/drive/MyDrive/faceforensics++/Adversarial_attacked_sequences/QueryBasedAttacks/BoundaryAttack/DeepfakeDetectionSet/Epsilon0.1/\"\n","NUM_CHANGE = 70\n","\n","for video_path in tqdm(glob.glob(os.path.join(src_dir, \"*.mp4\"))):\n","  fname = os.path.basename(video_path)\n","  if os.path.exists(output_dir + fname):\n","    continue\n","  Process_Video(src_dir + fname, output_dir + fname, NUM_CHANGE)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hK0RsjDXjOHA","outputId":"14ef22f5-e3a7-4084-d825-01f51884d20c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/3068 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["Loaded 1620 frames (W x H = 1920 x 1080), fps=24.0\n","Instantiated BoundaryAttack.\n","Patched attack.is_adversarial and attack.distance to use your detector.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNeXt50_32X4D_Weights.IMAGENET1K_V1`. You can also use `weights=ResNeXt50_32X4D_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["Frame 0: attack applied (q=5).\n","Frame 1: attack applied (q=5).\n","Frame 2: attack applied (q=5).\n","Frame 3: attack applied (q=5).\n","Mean Squared Error: tensor([197027.5625])\n","Calls: 2\n","Frame 4: attack applied (q=5).\n","Frame 5: attack applied (q=5).\n","Frame 6: attack applied (q=5).\n","Mean Squared Error: tensor([210277.5156])\n","Calls: 4\n","Frame 7: attack applied (q=5).\n","Frame 8: attack applied (q=5).\n"]}]}]}