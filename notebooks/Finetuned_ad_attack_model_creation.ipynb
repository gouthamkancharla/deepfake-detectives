{"cells":[{"cell_type":"markdown","source":["Set as GPU before running."],"metadata":{"id":"SVyanlHtNq7a"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nyhRlZbbNpc1","executionInfo":{"status":"ok","timestamp":1764040139651,"user_tz":300,"elapsed":15088,"user":{"displayName":"Jessie Kim","userId":"10055995841985743345"}},"outputId":"7268790a-2c04-4be4-d6f6-437613d673a9"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pip3 install face_recognition"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nUSYMpnQOARa","executionInfo":{"status":"ok","timestamp":1764040167721,"user_tz":300,"elapsed":28067,"user":{"displayName":"Jessie Kim","userId":"10055995841985743345"}},"outputId":"6b75a3c5-11ca-4869-a371-9508198d1ac4"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting face_recognition\n","  Downloading face_recognition-1.3.0-py2.py3-none-any.whl.metadata (21 kB)\n","Collecting face-recognition-models>=0.3.0 (from face_recognition)\n","  Downloading face_recognition_models-0.3.0.tar.gz (100.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.1/100.1 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.12/dist-packages (from face_recognition) (8.3.1)\n","Requirement already satisfied: dlib>=19.7 in /usr/local/lib/python3.12/dist-packages (from face_recognition) (19.24.6)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from face_recognition) (2.0.2)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from face_recognition) (11.3.0)\n","Downloading face_recognition-1.3.0-py2.py3-none-any.whl (15 kB)\n","Building wheels for collected packages: face-recognition-models\n","  Building wheel for face-recognition-models (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for face-recognition-models: filename=face_recognition_models-0.3.0-py2.py3-none-any.whl size=100566166 sha256=538d3b5283333aef944a7c8138ef0546f2a21c1bd987aef0622084db808af227\n","  Stored in directory: /root/.cache/pip/wheels/8f/47/c8/f44c5aebb7507f7c8a2c0bd23151d732d0f0bd6884ad4ac635\n","Successfully built face-recognition-models\n","Installing collected packages: face-recognition-models, face_recognition\n","Successfully installed face-recognition-models-0.3.0 face_recognition-1.3.0\n"]}]},{"cell_type":"code","source":["!pip install torch torchvision timm pandas scikit-learn matplotlib seaborn opencv-python\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"twzD03EkRRpy","executionInfo":{"status":"ok","timestamp":1764040179848,"user_tz":300,"elapsed":12125,"user":{"displayName":"Jessie Kim","userId":"10055995841985743345"}},"outputId":"ab08cf09-5515-4f90-bcfb-1d120e926643"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n","Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (1.0.22)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm) (6.0.3)\n","Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from timm) (0.36.0)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm) (0.7.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (2.32.4)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (4.67.1)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (1.2.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2025.11.12)\n"]}]},{"cell_type":"code","source":["# import os\n","# import numpy as np\n","# import torch\n","# from torch.utils.data import Dataset, DataLoader\n","# from torchvision import transforms, models\n","# from torch import nn\n","# from sklearn.metrics import f1_score, roc_curve, auc, confusion_matrix\n","# import matplotlib.pyplot as plt\n","# import pandas as pd\n","# import seaborn as sn\n","# import cv2\n","# import sys\n","\n","# # --------------------------------------------\n","# # Model Definition\n","# # --------------------------------------------\n","# class Model(nn.Module):\n","#     def __init__(self, num_classes, latent_dim=2048, lstm_layers=1, hidden_dim=2048, bidirectional=False):\n","#         super(Model, self).__init__()\n","#         base_model = models.resnext50_32x4d(weights=None)\n","#         self.cnn = nn.Sequential(*list(base_model.children())[:-2])\n","#         self.avgpool = nn.AdaptiveAvgPool2d(1)\n","#         self.lstm = nn.LSTM(latent_dim, hidden_dim, lstm_layers, bidirectional=bidirectional)\n","#         self.dp = nn.Dropout(0.4)\n","#         self.linear = nn.Linear(hidden_dim, num_classes)\n","\n","#     def forward(self, x):\n","#         batch_size, seq_length, c, h, w = x.shape\n","#         x = x.view(batch_size * seq_length, c, h, w)\n","#         fmap = self.cnn(x)\n","#         x = self.avgpool(fmap)\n","#         x = x.view(batch_size, seq_length, 2048)\n","#         x_lstm, _ = self.lstm(x)\n","#         x = torch.mean(x_lstm, dim=1)\n","#         return fmap, self.dp(self.linear(x))\n","\n","# # --------------------------------------------\n","# # Dataset with padding and skip zero frames\n","# # --------------------------------------------\n","# class video_dataset(Dataset):\n","#     def __init__(self, video_names, labels, sequence_length=20, transform=None):\n","#         self.video_names = video_names\n","#         self.labels = labels\n","#         self.count = sequence_length\n","#         self.transform = transform\n","\n","#     def __len__(self):\n","#         return len(self.video_names)\n","\n","#     def __getitem__(self, idx):\n","#         video_path = self.video_names[idx]\n","#         frames = []\n","#         temp_video = os.path.basename(video_path)\n","#         label = self.labels.loc[self.labels[\"file\"] == temp_video, \"label\"].values[0]\n","\n","#         for frame in self.frame_extract(video_path):\n","#             frames.append(self.transform(frame))\n","#             if len(frames) == self.count:\n","#                 break\n","\n","#         if len(frames) == 0:\n","#             # Skip videos with 0 frames\n","#             return self.__getitem__((idx + 1) % len(self.video_names))\n","\n","#         frames = torch.stack(frames)\n","\n","#         # Pad short videos by repeating last frame\n","#         if frames.shape[0] < self.count:\n","#             pad_count = self.count - frames.shape[0]\n","#             last_frame = frames[-1].unsqueeze(0).repeat(pad_count, 1, 1, 1)\n","#             frames = torch.cat([frames, last_frame], dim=0)\n","\n","#         return frames, label\n","\n","#     def frame_extract(self, path):\n","#         vidObj = cv2.VideoCapture(path)\n","#         success = True\n","#         while success:\n","#             success, image = vidObj.read()\n","#             if success:\n","#                 yield image\n","\n","# # --------------------------------------------\n","# # Utilities\n","# # --------------------------------------------\n","# def train_epoch(epoch, num_epochs, data_loader, model, criterion, optimizer):\n","#     model.train()\n","#     losses = AverageMeter()\n","#     accuracies = AverageMeter()\n","#     for i, (inputs, targets) in enumerate(data_loader):\n","#         device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","#         inputs, targets = inputs.to(device), targets.long().to(device)\n","#         _, outputs = model(inputs)\n","#         loss = criterion(outputs, targets)\n","#         acc = calculate_accuracy(outputs, targets)\n","#         losses.update(loss.item(), inputs.size(0))\n","#         accuracies.update(acc, inputs.size(0))\n","#         optimizer.zero_grad()\n","#         loss.backward()\n","#         optimizer.step()\n","#         sys.stdout.write(\n","#             f\"\\r[Epoch {epoch}/{num_epochs}] [Batch {i}/{len(data_loader)}] [Train Loss: {losses.avg:.4f}, Train Acc: {accuracies.avg:.2f}%]\"\n","#         )\n","#     print()\n","#     return losses.avg, accuracies.avg\n","\n","# def test(epoch, model, data_loader, criterion):\n","#     model.eval()\n","#     losses = AverageMeter()\n","#     accuracies = AverageMeter()\n","#     pred, true, probs_list = [], [], []\n","#     with torch.no_grad():\n","#         for i, (inputs, targets) in enumerate(data_loader):\n","#             device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","#             inputs, targets = inputs.to(device), targets.long().to(device)\n","#             _, outputs = model(inputs)\n","#             probs = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n","#             loss = criterion(outputs, targets)\n","#             acc = calculate_accuracy(outputs, targets)\n","#             _, p = torch.max(outputs, 1)\n","#             true += targets.cpu().numpy().tolist()\n","#             pred += p.cpu().numpy().tolist()\n","#             probs_list.extend(probs.tolist())\n","#             losses.update(loss.item(), inputs.size(0))\n","#             accuracies.update(acc, inputs.size(0))\n","#     return true, pred, probs_list, losses.avg, accuracies.avg\n","\n","# def calculate_accuracy(outputs, targets):\n","#     _, pred = outputs.max(1)\n","#     correct = pred.eq(targets).sum().item()\n","#     return 100 * correct / targets.size(0)\n","\n","# class AverageMeter:\n","#     def __init__(self):\n","#         self.reset()\n","#     def reset(self):\n","#         self.val = self.avg = self.sum = self.count = 0\n","#     def update(self, val, n=1):\n","#         self.val = val\n","#         self.sum += val * n\n","#         self.count += n\n","#         self.avg = self.sum / self.count\n","\n","# def save_checkpoint(state, filename):\n","#     os.makedirs(os.path.dirname(filename), exist_ok=True)\n","#     torch.save(state, filename)\n","#     print(f\"\\nCheckpoint saved: {filename}\")\n","\n","# def read_list(txt_path):\n","#     with open(txt_path, 'r') as f:\n","#         return [line.strip() for line in f.readlines()]\n","\n","# def assign_label(path):\n","#     path_low = path.lower()\n","#     if \"fake\" in path_low or \"deepfake\" in path_low or \"manipulated\" in path_low:\n","#         return 1\n","#     return 0\n","\n","# # --------------------------------------------\n","# # Parameters\n","# # --------------------------------------------\n","# SEQ_LEN = 20\n","# BATCH_SIZE = 4\n","# NUM_EPOCHS = 20\n","# LR = 1e-5\n","# IM_SIZE = 112\n","\n","# # --------------------------------------------\n","# # Dataset splits\n","# # --------------------------------------------\n","# train_files = read_list(\"/content/drive/MyDrive/deepfake_detection_project/Dataset_split/adversarial_splits/train.txt\")\n","# val_files = read_list(\"/content/drive/MyDrive/deepfake_detection_project/Dataset_split/adversarial_splits/val.txt\")\n","\n","# train_labels = pd.DataFrame({\"file\":[os.path.basename(p) for p in train_files],\n","#                              \"label\":[assign_label(p) for p in train_files]})\n","# val_labels = pd.DataFrame({\"file\":[os.path.basename(p) for p in val_files],\n","#                            \"label\":[assign_label(p) for p in val_files]})\n","\n","# # --------------------------------------------\n","# # Transforms\n","# # --------------------------------------------\n","# train_transform = transforms.Compose([transforms.ToPILImage(),\n","#                                       transforms.Resize((IM_SIZE, IM_SIZE)),\n","#                                       transforms.ToTensor()])\n","# val_transform = train_transform\n","\n","# # --------------------------------------------\n","# # Dataset and loaders\n","# # --------------------------------------------\n","# train_dataset = video_dataset(train_files, train_labels, SEQ_LEN, transform=train_transform)\n","# val_dataset   = video_dataset(val_files, val_labels, SEQ_LEN, transform=val_transform)\n","\n","# train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n","# val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n","\n","# # --------------------------------------------\n","# # Model, optimizer, criterion\n","# # --------------------------------------------\n","# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# model = Model(2).to(device)\n","# optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n","# criterion = nn.CrossEntropyLoss().to(device)\n","\n","# # --------------------------------------------\n","# # Training loop\n","# # --------------------------------------------\n","# train_loss_avg, train_acc_list = [], []\n","# val_loss_avg, val_acc_list = [], []\n","\n","# val_acc_file = \"/content/drive/MyDrive/deepfake_detection_project/Fine-tuning/val_accuracy.txt\"\n","\n","# for epoch in range(1, NUM_EPOCHS + 1):\n","#     train_loss, train_acc = train_epoch(epoch, NUM_EPOCHS, train_loader, model, criterion, optimizer)\n","#     train_loss_avg.append(train_loss)\n","#     train_acc_list.append(train_acc)\n","\n","#     y_true, y_pred, y_probs, val_loss, val_acc = test(epoch, model, val_loader, criterion)\n","#     val_loss_avg.append(val_loss)\n","#     val_acc_list.append(val_acc)\n","\n","#     # Print and save validation accuracy\n","#     print(f\"Epoch {epoch} Validation Accuracy: {val_acc:.2f}%\")\n","#     with open(val_acc_file, \"a\") as f:\n","#         f.write(f\"Epoch {epoch}: {val_acc:.2f}%\\n\")\n","\n","#     # Save model checkpoint\n","#     save_checkpoint({\n","#         \"epoch\": epoch,\n","#         \"model_state\": model.state_dict(),\n","#         \"optimizer_state\": optimizer.state_dict(),\n","#         \"train_loss\": train_loss,\n","#         \"train_acc\": train_acc,\n","#         \"val_loss\": val_loss,\n","#         \"val_acc\": val_acc\n","#     }, filename=f\"/content/drive/MyDrive/deepfake_detection_project/Fine-tuning/models/model_epoch_{epoch}.pt\")\n"],"metadata":{"id":"Kkmfp08BN2Q9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import numpy as np\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, models\n","from torch import nn\n","from sklearn.metrics import f1_score, roc_curve, auc, confusion_matrix\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import seaborn as sn\n","import cv2\n","import sys\n","\n","\n","\n","# --------------------------------------------\n","# Model Definition\n","# --------------------------------------------\n","class Model(nn.Module):\n","    def __init__(self, num_classes, latent_dim=2048, lstm_layers=1, hidden_dim=2048, bidirectional=False):\n","        super(Model, self).__init__()\n","        base_model = models.resnext50_32x4d(weights=None)\n","        self.cnn = nn.Sequential(*list(base_model.children())[:-2])\n","        self.avgpool = nn.AdaptiveAvgPool2d(1)\n","        self.lstm = nn.LSTM(latent_dim, hidden_dim, lstm_layers, bidirectional=bidirectional)\n","        self.dp = nn.Dropout(0.4)\n","        self.linear = nn.Linear(hidden_dim, num_classes)\n","\n","    def forward(self, x):\n","        batch_size, seq_length, c, h, w = x.shape\n","        x = x.view(batch_size * seq_length, c, h, w)\n","        fmap = self.cnn(x)\n","        x = self.avgpool(fmap)\n","        x = x.view(batch_size, seq_length, 2048)\n","        x_lstm, _ = self.lstm(x)\n","        x = torch.mean(x_lstm, dim=1)\n","        return fmap, self.dp(self.linear(x))\n","\n","# --------------------------------------------\n","# Dataset with padding and skip zero frames\n","# --------------------------------------------\n","class video_dataset(Dataset):\n","    def __init__(self, video_names, labels, sequence_length=20, transform=None):\n","        self.video_names = video_names\n","        self.labels = labels\n","        self.count = sequence_length\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.video_names)\n","\n","    def __getitem__(self, idx):\n","        video_path = self.video_names[idx]\n","        frames = []\n","        temp_video = os.path.basename(video_path)\n","        label = self.labels.loc[self.labels[\"file\"] == temp_video, \"label\"].values[0]\n","\n","        for frame in self.frame_extract(video_path):\n","            frames.append(self.transform(frame))\n","            if len(frames) == self.count:\n","                break\n","\n","        if len(frames) == 0:\n","            # Skip videos with 0 frames\n","            return self.__getitem__((idx + 1) % len(self.video_names))\n","\n","        frames = torch.stack(frames)\n","\n","        # Pad short videos by repeating last frame\n","        if frames.shape[0] < self.count:\n","            pad_count = self.count - frames.shape[0]\n","            last_frame = frames[-1].unsqueeze(0).repeat(pad_count, 1, 1, 1)\n","            frames = torch.cat([frames, last_frame], dim=0)\n","\n","        return frames, label\n","\n","    def frame_extract(self, path):\n","        vidObj = cv2.VideoCapture(path)\n","        success = True\n","        while success:\n","            success, image = vidObj.read()\n","            if success:\n","                yield image\n","\n","# --------------------------------------------\n","# Utilities\n","# --------------------------------------------\n","def train_epoch(epoch, num_epochs, data_loader, model, criterion, optimizer):\n","    model.train()\n","    losses = AverageMeter()\n","    accuracies = AverageMeter()\n","    for i, (inputs, targets) in enumerate(data_loader):\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        inputs, targets = inputs.to(device), targets.long().to(device)\n","        _, outputs = model(inputs)\n","        loss = criterion(outputs, targets)\n","        acc = calculate_accuracy(outputs, targets)\n","        losses.update(loss.item(), inputs.size(0))\n","        accuracies.update(acc, inputs.size(0))\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        sys.stdout.write(\n","            f\"\\r[Epoch {epoch}/{num_epochs}] [Batch {i}/{len(data_loader)}] [Train Loss: {losses.avg:.4f}, Train Acc: {accuracies.avg:.2f}%]\"\n","        )\n","    print()\n","    return losses.avg, accuracies.avg\n","\n","def test(epoch, model, data_loader, criterion):\n","    model.eval()\n","    losses = AverageMeter()\n","    accuracies = AverageMeter()\n","    pred, true, probs_list = [], [], []\n","    with torch.no_grad():\n","        for i, (inputs, targets) in enumerate(data_loader):\n","            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","            inputs, targets = inputs.to(device), targets.long().to(device)\n","            _, outputs = model(inputs)\n","            probs = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n","            loss = criterion(outputs, targets)\n","            acc = calculate_accuracy(outputs, targets)\n","            _, p = torch.max(outputs, 1)\n","            true += targets.cpu().numpy().tolist()\n","            pred += p.cpu().numpy().tolist()\n","            probs_list.extend(probs.tolist())\n","            losses.update(loss.item(), inputs.size(0))\n","            accuracies.update(acc, inputs.size(0))\n","    return true, pred, probs_list, losses.avg, accuracies.avg\n","\n","def calculate_accuracy(outputs, targets):\n","    _, pred = outputs.max(1)\n","    correct = pred.eq(targets).sum().item()\n","    return 100 * correct / targets.size(0)\n","\n","class AverageMeter:\n","    def __init__(self):\n","        self.reset()\n","    def reset(self):\n","        self.val = self.avg = self.sum = self.count = 0\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","def save_checkpoint(state, filename):\n","    os.makedirs(os.path.dirname(filename), exist_ok=True)\n","    torch.save(state, filename)\n","    print(f\"\\nCheckpoint saved: {filename}\")\n","\n","def read_list(txt_path):\n","    with open(txt_path, 'r') as f:\n","        return [line.strip() for line in f.readlines()]\n","\n","def assign_label(path):\n","    path_low = path.lower()\n","    if \"fake\" in path_low or \"deepfake\" in path_low or \"manipulated\" in path_low:\n","        return 1\n","    return 0\n","\n","\n","SEQ_LEN = 20\n","BATCH_SIZE = 4\n","NUM_EPOCHS = 50 # update to the max epoch you want\n","LR = 1e-5\n","IM_SIZE = 112\n","\n","# --------------------------------------------\n","# Dataset splits\n","# --------------------------------------------\n","train_files = read_list(\"/content/drive/MyDrive/deepfake_detection_project/Dataset_split/adversarial_splits/train.txt\")\n","val_files = read_list(\"/content/drive/MyDrive/deepfake_detection_project/Dataset_split/adversarial_splits/val.txt\")\n","\n","train_labels = pd.DataFrame({\"file\":[os.path.basename(p) for p in train_files],\n","                             \"label\":[assign_label(p) for p in train_files]})\n","val_labels = pd.DataFrame({\"file\":[os.path.basename(p) for p in val_files],\n","                           \"label\":[assign_label(p) for p in val_files]})\n","\n","# --------------------------------------------\n","# Transforms\n","# --------------------------------------------\n","train_transform = transforms.Compose([transforms.ToPILImage(),\n","                                      transforms.Resize((IM_SIZE, IM_SIZE)),\n","                                      transforms.ToTensor()])\n","val_transform = train_transform\n","\n","# --------------------------------------------\n","# Dataset and loaders\n","# --------------------------------------------\n","train_dataset = video_dataset(train_files, train_labels, SEQ_LEN, transform=train_transform)\n","val_dataset   = video_dataset(val_files, val_labels, SEQ_LEN, transform=val_transform)\n","\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n","val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n","\n","# --------------------------------------------\n","# Model, optimizer, criterion\n","# --------------------------------------------\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = Model(2).to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n","criterion = nn.CrossEntropyLoss().to(device)\n","\n","# Load checkpoint\n","checkpoint_path = \"/content/drive/MyDrive/deepfake_detection_project/Fine-tuning/models/model_epoch_43.pt\"\n","checkpoint = torch.load(checkpoint_path, map_location=device)\n","model.load_state_dict(checkpoint[\"model_state\"])\n","optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n","start_epoch = checkpoint[\"epoch\"] + 1  # continue from next epoch\n","\n","print(f\"Resuming training from epoch {start_epoch}\")\n","\n","\n","# Training loop\n","train_loss_avg, train_acc_list = [], []\n","val_loss_avg, val_acc_list = [], []\n","\n","val_acc_file = \"/content/drive/MyDrive/deepfake_detection_project/Fine-tuning/val_accuracy.txt\"\n","\n","for epoch in range(start_epoch, NUM_EPOCHS + 1):\n","    train_loss, train_acc = train_epoch(epoch, NUM_EPOCHS, train_loader, model, criterion, optimizer)\n","    train_loss_avg.append(train_loss)\n","    train_acc_list.append(train_acc)\n","\n","    y_true, y_pred, y_probs, val_loss, val_acc = test(epoch, model, val_loader, criterion)\n","    val_loss_avg.append(val_loss)\n","    val_acc_list.append(val_acc)\n","\n","    # Print and save validation accuracy\n","    print(f\"Epoch {epoch} Validation Accuracy: {val_acc:.2f}%\")\n","    with open(val_acc_file, \"a\") as f:\n","        f.write(f\"Epoch {epoch}: {val_acc:.2f}%\\n\")\n","\n","    # Save model checkpoint\n","    save_checkpoint({\n","        \"epoch\": epoch,\n","        \"model_state\": model.state_dict(),\n","        \"optimizer_state\": optimizer.state_dict(),\n","        \"train_loss\": train_loss,\n","        \"train_acc\": train_acc,\n","        \"val_loss\": val_loss,\n","        \"val_acc\": val_acc\n","    }, filename=f\"/content/drive/MyDrive/deepfake_detection_project/Fine-tuning/models/model_epoch_{epoch}.pt\")\n"],"metadata":{"id":"ppBJeqozO45r","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764030620518,"user_tz":300,"elapsed":4075430,"user":{"displayName":"J Kim","userId":"11919506894406427579"}},"outputId":"1890e772-5b74-48a5-a96a-813d315135e6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Resuming training from epoch 44\n","[Epoch 44/50] [Batch 249/250] [Train Loss: 0.5064, Train Acc: 73.90%]\n","Epoch 44 Validation Accuracy: 61.00%\n","\n","Checkpoint saved: /content/drive/MyDrive/deepfake_detection_project/Fine-tuning/models/model_epoch_44.pt\n","[Epoch 45/50] [Batch 249/250] [Train Loss: 0.4963, Train Acc: 75.20%]\n","Epoch 45 Validation Accuracy: 61.50%\n","\n","Checkpoint saved: /content/drive/MyDrive/deepfake_detection_project/Fine-tuning/models/model_epoch_45.pt\n","[Epoch 46/50] [Batch 249/250] [Train Loss: 0.5067, Train Acc: 74.90%]\n","Epoch 46 Validation Accuracy: 61.00%\n","\n","Checkpoint saved: /content/drive/MyDrive/deepfake_detection_project/Fine-tuning/models/model_epoch_46.pt\n","[Epoch 47/50] [Batch 249/250] [Train Loss: 0.4811, Train Acc: 75.80%]\n","Epoch 47 Validation Accuracy: 62.00%\n","\n","Checkpoint saved: /content/drive/MyDrive/deepfake_detection_project/Fine-tuning/models/model_epoch_47.pt\n","[Epoch 48/50] [Batch 249/250] [Train Loss: 0.4801, Train Acc: 74.80%]\n","Epoch 48 Validation Accuracy: 62.50%\n","\n","Checkpoint saved: /content/drive/MyDrive/deepfake_detection_project/Fine-tuning/models/model_epoch_48.pt\n","[Epoch 49/50] [Batch 249/250] [Train Loss: 0.5020, Train Acc: 73.80%]\n","Epoch 49 Validation Accuracy: 63.50%\n","\n","Checkpoint saved: /content/drive/MyDrive/deepfake_detection_project/Fine-tuning/models/model_epoch_49.pt\n","[Epoch 50/50] [Batch 249/250] [Train Loss: 0.5087, Train Acc: 74.70%]\n","Epoch 50 Validation Accuracy: 59.00%\n","\n","Checkpoint saved: /content/drive/MyDrive/deepfake_detection_project/Fine-tuning/models/model_epoch_50.pt\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1kHqOwqEriAK2byxd3B1rQyuEhv3sw6-s","timestamp":1763958580844},{"file_id":"1a6ryD9-lSd-bjwcqmlWfSJmmP0kR5cuh","timestamp":1763879539551}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}