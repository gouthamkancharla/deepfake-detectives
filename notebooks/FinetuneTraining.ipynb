{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMWuS8LmRycaHePsQoNyr7e"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"gqJ8QWB-Ccna"},"outputs":[],"source":["import os\n","import cv2\n","import random\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","import timm  # Recommended for Xception\n","from tqdm import tqdm\n","\n","# ====================================================\n","# 1. CONFIGURATION\n","# ====================================================\n","CONFIG = {\n","    \"gpu_id\": 0,\n","    \"num_workers\": 4,\n","    \"im_size\": 299,        # Standard input size for Xception\n","    \"batch_size\": 16,      # Can often use larger batch size than Madry since no attack overhead\n","    \"epochs\": 20,\n","    \"lr\": 1e-4,\n","    \"sequence_length\": 5,  # Number of frames to extract per video\n","    \"base_path\": \"/content/drive/MyDrive/csc490/code_and_datasets/video_splits_output\", # Path to txt files\n","    \"checkpoint_dir\": \"/content/drive/MyDrive/csc490/code_and_datasets/checkpoints\"\n","}\n","\n","device = torch.device(f\"cuda:{CONFIG['gpu_id']}\" if torch.cuda.is_available() else \"cpu\")\n","os.makedirs(CONFIG[\"checkpoint_dir\"], exist_ok=True)\n","\n","# ====================================================\n","# 2. DATASET & TRANSFORMS\n","# ====================================================\n","\n","# We keep the same transform logic as Madry for consistency.\n","# Images are loaded in [0, 1] range. Normalization happens before the model.\n","train_transforms = transforms.Compose([\n","    transforms.ToPILImage(),\n","    transforms.Resize((CONFIG['im_size'], CONFIG['im_size'])),\n","    transforms.ToTensor(),\n","])\n","\n","test_transforms = transforms.Compose([\n","    transforms.ToPILImage(),\n","    transforms.Resize((CONFIG['im_size'], CONFIG['im_size'])),\n","    transforms.ToTensor(),\n","])\n","\n","class DeepfakeVideoDataset(Dataset):\n","    def __init__(self, file_list_path, sequence_length, transform=None):\n","        # Read video paths from the text file\n","        # ASSUMPTION: 'train.txt' contains Original, Deepfake, AND Adversarial video paths\n","        with open(file_list_path, 'r') as f:\n","            self.video_paths = [line.strip() for line in f.readlines() if line.strip()]\n","\n","        self.sequence_length = sequence_length\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.video_paths)\n","\n","    def get_label(self, path):\n","        # Logic:\n","        # - \"original\" folder -> Label 0 (Real)\n","        # - \"deepfakes\", \"adversarial\", \"neuraltextures\" etc. -> Label 1 (Fake)\n","        if \"original\" in path.lower():\n","            return 0\n","        else:\n","            return 1\n","\n","    def __getitem__(self, idx):\n","        video_path = self.video_paths[idx]\n","        label = self.get_label(video_path)\n","\n","        cap = cv2.VideoCapture(video_path)\n","        frames = []\n","        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","\n","        # Random sampling\n","        if frame_count > self.sequence_length:\n","            indices = sorted(random.sample(range(frame_count), self.sequence_length))\n","        else:\n","            indices = list(range(frame_count))\n","\n","        for i in range(frame_count):\n","            ret, frame = cap.read()\n","            if not ret: break\n","            if i in indices:\n","                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","                if self.transform:\n","                    frame = self.transform(frame)\n","                frames.append(frame)\n","                if len(frames) >= self.sequence_length:\n","                    break\n","        cap.release()\n","\n","        # Padding\n","        if len(frames) == 0:\n","            return torch.zeros((self.sequence_length, 3, CONFIG['im_size'], CONFIG['im_size'])), label\n","\n","        while len(frames) < self.sequence_length:\n","            frames.append(frames[-1])\n","\n","        return torch.stack(frames), label\n","\n","# ====================================================\n","# 3. MODEL & NORMALIZATION HELPER\n","# ====================================================\n","\n","# Manual Normalization (Same as Madry code for consistency)\n","def normalize_batch(imgs):\n","    mean = torch.tensor([0.485, 0.456, 0.406], device=imgs.device).view(1, 3, 1, 1)\n","    std = torch.tensor([0.229, 0.224, 0.225], device=imgs.device).view(1, 3, 1, 1)\n","    return (imgs - mean) / std\n","\n","def get_model():\n","    # Load Pretrained Xception\n","    model = timm.create_model('xception', pretrained=True, num_classes=2)\n","    return model.to(device)\n","\n","# ====================================================\n","# 4. MAIN TRAINING LOOP (Standard Fine-tuning)\n","# ====================================================\n","\n","def main():\n","    # 1. Load Data\n","    # 'train.txt' MUST include the paths to the pre-generated adversarial videos\n","    train_dataset = DeepfakeVideoDataset(os.path.join(CONFIG['base_path'], \"train.txt\"), CONFIG['sequence_length'], train_transforms)\n","    val_dataset = DeepfakeVideoDataset(os.path.join(CONFIG['base_path'], \"val.txt\"), CONFIG['sequence_length'], test_transforms)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=CONFIG['num_workers'])\n","    val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=CONFIG['num_workers'])\n","\n","    print(f\"Dataset Loaded: Train {len(train_dataset)} | Val {len(val_dataset)}\")\n","    print(\"NOTE: Ensure 'train.txt' contains mixed data (Original + Deepfake + Adversarial)\")\n","\n","    # 2. Model & Optimizer\n","    model = get_model()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG['lr'])\n","    criterion = nn.CrossEntropyLoss()\n","\n","    best_acc = 0.0\n","\n","    # 3. Training Loop\n","    for epoch in range(1, CONFIG['epochs'] + 1):\n","        model.train()\n","        train_loss = 0\n","        correct = 0\n","        total = 0\n","\n","        print(f\"\\nEpoch {epoch}/{CONFIG['epochs']} - Fine-tuning Started...\")\n","\n","        for inputs, labels in tqdm(train_loader):\n","            # Flatten inputs: (B, Seq, C, H, W) -> (B*Seq, C, H, W)\n","            b, s, c, h, w = inputs.shape\n","            inputs = inputs.view(b * s, c, h, w).to(device)\n","            labels = labels.repeat_interleave(s).to(device)\n","\n","            # --- DIFFERENCE IS HERE ---\n","            # No PGD Attack generation.\n","            # We assume 'inputs' already contains adversarial examples loaded from disk.\n","\n","            # Normalize and Forward\n","            outputs = model(normalize_batch(inputs))\n","            loss = criterion(outputs, labels)\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            train_loss += loss.item() * inputs.size(0)\n","            _, predicted = outputs.max(1)\n","            total += labels.size(0)\n","            correct += predicted.eq(labels).sum().item()\n","\n","        epoch_loss = train_loss / total\n","        epoch_acc = 100. * correct / total\n","        print(f\"Train Loss: {epoch_loss:.4f} | Train Acc (Mixed): {epoch_acc:.2f}%\")\n","\n","        # 4. Validation\n","        model.eval()\n","        val_correct = 0\n","        val_total = 0\n","        with torch.no_grad():\n","            for inputs, labels in val_loader:\n","                b, s, c, h, w = inputs.shape\n","                inputs = inputs.view(b * s, c, h, w).to(device)\n","                labels = labels.repeat_interleave(s).to(device)\n","\n","                outputs = model(normalize_batch(inputs))\n","                _, predicted = outputs.max(1)\n","                val_total += labels.size(0)\n","                val_correct += predicted.eq(labels).sum().item()\n","\n","        val_acc = 100. * val_correct / val_total\n","        print(f\"Val Acc: {val_acc:.2f}%\")\n","\n","        # 5. Save Checkpoint\n","        if val_acc > best_acc:\n","            best_acc = val_acc\n","            save_path = os.path.join(CONFIG['checkpoint_dir'], f\"finetune_best_model.pth\")\n","            torch.save(model.state_dict(), save_path)\n","            print(f\"Saved Best Model to {save_path}\")\n","\n","if __name__ == \"__main__\":\n","    main()"]}]}